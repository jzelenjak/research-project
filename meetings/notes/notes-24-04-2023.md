# Notes for Week 1 meeting

**Date**: 24-04-2023 (Week 4.1)

**Time**: 9:00 - 10:00

**Location**: Echo, Computer Science offices (2nd floor)

## Notes

### General questions

### Thinking out of the box

- RP is not as much implementation centric as SP; it is more about using the scientific method
- Successful running project is not necessary, but answering the question is
- **Bare minimum**: answer the question
- **High grade**: answer the question well, being very systematic about it (back up with literature), do have a reason for doing stuff (not "This is what we could do given the time")
- Do limit the scope

- If a question is too difficult, it is still possible to get a high grade
- You fail 10 times and pass one time
- 10 weeks might not be sufficient
- Planning might be insufficient, not communicating early enough

### Asking questions outside the meetings

- Asking on MM is ok
- Stuck 30 minutes or more - ask for help
- First google / ask ChatGPT :)
- Ask other group members
- Update the question on MM if you have figured out the answer in the meantime

### Midterm presentation

- We don't have the date; seems like Monday is mandatory
- We will ask during the kick-off meeting

### Research plan

- Research plan is also meant for us to think about the steps/literature, to understand out project
- Living document, but you decide if you want to update it
- Maybe you can create some MoSCoW, but it is more flexible
- How would you go about answering? Run the tool, change parameter settings, evaluation criteria. This helps in figuring out whether you have answered the question
- **Keep some days as buffers!** (e.g. if you are stuck, or if your experiments take longer)
- Not a lot of time for actual research (first two weeks), then writing the report and answering the questions

### Implementation-specific questions

#### Prioritising critical episodes

- Priority level is the default way of checking
- You also want to know alerts that indicate the start of an attack (e.g. scanning), they will have low-severity
- No ground truth
- "This signature was triggered, let me check if this attack has been successful. There are false positives" (there are no ways of checking)
- Which kind of alerts do we want to prioritise (currently: frequency and severity)

#### SAGE: episodes

- Linear (uniform): a long episode
- **Start**: positive slope (increase in frequency of alerts)
- **Global minimum**: across the entire dataset, usually 0 (no alerts)
- This is just a heuristic
- If `w` is too big, all of it will be just one attack
- If `w` is too small, one peak might be spanned across multiple episodes

### Sink states

- No outgoing edges (not always): appear too infrequently
- *States are merged if the futures and pasts are similar*
- If an event occurs less than some count - no merge
- `State ID` tells about the context: states semantically mean different things (the way to reach them was different)
- `Sink states`: we didn't do any learning on it
- *Do we merge them or not?*
- `Symbol count`: states that are candidates to merging; no sufficient evidence to prevent a merge -> merge (we have observed enough evidence to think they are the same)
- `sink_count`: sinks are not touched at all
- For now they are kept as they are


- AGs are important, S-PDFA is just assisting thing
- We want to be able to identify semantically similar and different states

### Running SAGE

- Also clone the main branch with Flexfringe
- At the beginning you can use Docker to just see the graphs

### Questions related to research questions

#### Q1-Q3

- Evaluation criteria, change something and compare AGs
- `Size` (exists), `complexity` (reuse the ratio), `interpretability` (no metric right now), `completeness` (this is very hard; maybe look at individual attack paths and compare them: which one are longer, more complete)
- Q1-Q3 can collaborate, identify stuff like completeness at the beginning
- Q1: no suffixes (i.e. reversing)
- Creative part: e.g. how to make PDFA better? Suffix-based PDFA assumes we know the future. PDFA might be the actual solution but the problem is the low-severity states


- The main difference btw Q1-Q3 is the analysis
- Reasoning is different, although you can in principle copy the stuff from each other (not recommended, though)

#### Q2

- Learn the model from main model
- Some states might be lost but are still important
- We pick IDs and put in S-PDFA

- **Let's get rid of sinks by learning from them**
- What happens?

#### Comparing AGs (Q1-Q3)

- Pick AGs that are very different
- Automate this somehow
- Some graphs are very big (e.g. related to HTTP)

#### Q6: what input to use?

- Q6 is related to Q1-Q3 in terms of interpretability measures
- You can have two points: interpretability of the model and interpretability of AGs
- We know what a model does, but we don't really know about AGs
- Prioritise AGs as input (although you could also use alerts for additional insights)
- Humans can have only some amount of info in their working memory
- Cognitive chunks in AGs

### Research papers

- Running code
- Understand your question
- Then do the literature search
- Papers on Project Forum are enough to get started

- **Feedback for documents**: send at least 2 days in advance
- **Message on weekends**: don't expect it to be answered until Monday

## Appendix: Allocation of research questions

- `Q1` *How do the attack graphs compare to the baseline when the S-PDFA is swapped with a PDFA?* - Ioan
- `Q2` *What kind of attack graphs are generated as a result of merging the sink states with the main S-PDFA model?* - Jegor
- `Q3` *What kind of attack graphs are generated as a result of making the sink states loop with the last blue state of the main S-PDFA model?*- Alexandru
- `Q5` *Define a prioritization criteria for critical attack episodes, and compare against the baseline criteria. Does the proposed prioritization criteria discover more critical episodes?*- Senne
- `Q6` *How can the interpretability of the attack graphs be quantified?* - Vlad

